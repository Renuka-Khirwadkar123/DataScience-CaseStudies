{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd9551f6",
   "metadata": {},
   "source": [
    "Data loading and initial exploration\n",
    "\n",
    "Load the dataset into a pandas DataFrame\n",
    "Explore the first few rows to understand the structure of data\n",
    "Check the data types, summary statistics, and unique values of each column\n",
    "Identify any data quality issues or inconsistencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57933155",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"../CaseStudy3/SuperStore_Dataset.csv\")\n",
    "# Display the first 10 rows of the DataFrame    \n",
    "first_10_rows_data = df.head(10)\n",
    "print(f\"First 10 rows of data:\\n {first_10_rows_data} \\n\")\n",
    "# Display the last 10 rows of the DataFrame    \n",
    "last_10_rows_data = df.tail(10)\n",
    "print(f\"Last 10 rows of data: \\n {last_10_rows_data}\")\n",
    "#Display the data types of each column in the DataFrame\n",
    "data_types_in_datasets=df.dtypes\n",
    "print(f\"Data types in the dataset:\\n {data_types_in_datasets} \\n\")\n",
    "#display summary statistics of the DataFrame\n",
    "summary_statistics = df.describe()\n",
    "print(f\"Summary statistics of the dataset:\\n {summary_statistics} \\n\")\n",
    "#Display unique values in all columns of the DataFrame\n",
    "unique_values = {col: df[col].unique() for col in df.columns}\n",
    "print(f\"Unique values in each column:\\n {unique_values} \\n\")\n",
    "#describe the DataFrame\n",
    "data_information=df.info()\n",
    "print(f\"Data information:\\n {data_information} \\n\")\n",
    "#finding data quality issues\n",
    "missing_values = df.isnull().sum()\n",
    "print(f\"Missing values in each column:\\n {missing_values} \\n\")          \n",
    "# Display the number of duplicate rows in the DataFrame\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows in the dataset: {duplicate_rows} \\n\")\n",
    "# Display the number of rows and columns in the DataFrame\n",
    "number_of_rows, number_of_columns = df.shape        \n",
    "print(f\"Number of rows: {number_of_rows}, Number of columns: {number_of_columns} \\n\")\n",
    "# Display the column names in the DataFrame \n",
    "column_names = df.columns.tolist()\n",
    "print(f\"Column names in the dataset: {column_names} \\n\")    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33ff829",
   "metadata": {},
   "source": [
    "Handling Duplicates\n",
    "\n",
    "Identify and remove duplicate rows in the dataset\n",
    "Document the number of rows and distinct Order IDs affected by this operation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2a46c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check total rows before removing duplicates\n",
    "total_rows_before = df.shape[0]\n",
    "print(f\"Total rows before removing duplicates: {total_rows_before}\")\n",
    "\n",
    "# 2. Find duplicates\n",
    "duplicates = df[df.duplicated()]\n",
    "num_duplicates = duplicates.shape[0]\n",
    "print(f\"Total duplicate rows: {num_duplicates}\")\n",
    "\n",
    "# 3. Check how many unique Order IDs are involved in these duplicate rows\n",
    "affected_order_ids = duplicates['Order ID'].nunique()\n",
    "print(f\"Number of distinct Order IDs in duplicate rows: {affected_order_ids}\")\n",
    "\n",
    "# 4. Remove duplicate rows (keep first occurrence)\n",
    "df_cleaned = df.drop_duplicates()\n",
    "\n",
    "# 5. Verify new shape\n",
    "total_rows_after = df_cleaned.shape[0]\n",
    "print(f\"Total rows after removing duplicates: {total_rows_after}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28db0224",
   "metadata": {},
   "source": [
    "Date Handling\n",
    "\n",
    "Normalize the Order Date and Ship Date columns to ensure consistent date formats. Ensure that the format of the date is consistent across all rows\n",
    "Extract the year from the Order ID and compare it with the year in the Order Date. Correct any inconsistencies\n",
    "Document the number of rows and distinct Order IDs affected by these operations\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a6218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Order Date and Ship Date to datetime format\n",
    "df['Order Date'] = pd.to_datetime(df['Order Date'], errors='coerce')\n",
    "df['Ship Date'] = pd.to_datetime(df['Ship Date'], errors='coerce')\n",
    "# Check for any rows where Order Date or Ship Date could not be converted\n",
    "invalid_order_dates = df['Order Date'].isnull().sum()       \n",
    "invalid_ship_dates = df['Ship Date'].isnull().sum()\n",
    "print(f\"Invalid Order Dates: {invalid_order_dates}\")\n",
    "print(f\"Invalid Ship Dates: {invalid_ship_dates}\")\n",
    "# Check for any rows where Order Date is after Ship Date\n",
    "invalid_date_order_ship = df[df['Order Date'] > df['Ship Date']]        \n",
    "if not invalid_date_order_ship.empty:\n",
    "    print(f\"Rows with Order Date after Ship Date:\\n{invalid_date_order_ship[['Order ID', 'Order Date', 'Ship Date']]} \\n\")  \n",
    "else:\n",
    "    print(\"No rows found with Order Date after Ship Date.\\n\")\n",
    "\n",
    "#Extract the year from the Order ID and compare it with the year in the Order Date. Correct any inconsistencies\n",
    "df['Order Year'] = df['Order ID'].str.extract(r'(\\d{4})').astype(int)\n",
    "df['Order Date Year'] = df['Order Date'].dt.year    \n",
    "inconsistent_years = df[df['Order Year'] != df['Order Date Year']]\n",
    "if not inconsistent_years.empty:\n",
    "    print(f\"Inconsistent years found:\\n{inconsistent_years[['Order ID', 'Order Year', 'Order Date Year']]} \\n\") \n",
    "else:\n",
    "    print(\"No inconsistencies found between Order ID year and Order Date year.\\n\")\n",
    "\n",
    "#Document the number of rows and distinct Order IDs affected by these operations\n",
    "affected_rows = inconsistent_years.shape[0]\n",
    "affected_order_ids_count = inconsistent_years['Order ID'].nunique() \n",
    "print(f\"Number of affected rows: {affected_rows}\")\n",
    "print(f\"Number of distinct affected Order IDs: {affected_order_ids_count} \\n\")  \n",
    "\n",
    "# Save the cleaned DataFrame to a new CSV file\n",
    "df_cleaned.to_csv(\"../CaseStudy3/SuperStore_Cleaned_Dataset.csv\", index=False)\n",
    "print(\"Cleaned dataset saved to 'SuperStore_Cleaned_Dataset.csv'.\") \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc4ad8f",
   "metadata": {},
   "source": [
    "Imputation of Missing Values\n",
    "\n",
    "Impute missing values in the Ship Mode column using the calculated Days to Ship column\n",
    "Calculate Days to Ship as the difference between Ship Date and Order Date. If Days to Ship is 0, set Ship Mode to \"Same Day\"; if it is 7, set Ship Mode to \"Standard Class\"\n",
    "Impute missing values in the Quantity column using a method of your choice. Print the rationale for selecting the imputation method\n",
    "Document the number of rows and distinct Order IDs affected by these operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7783c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute missing values in the Ship Mode column using the calculated Days to Ship column\n",
    "df['Days to Ship'] = (df['Ship Date'] - df['Order Date']).dt.days\n",
    "df['Days to Ship'] = df['Days to Ship'].fillna(df['Days to Ship'].mean())\n",
    "# Impute missing values in the Ship Mode column\n",
    "df['Ship Mode'] = df['Ship Mode'].fillna(df['Ship Mode'].mode()[0])\n",
    "\n",
    "#Calculate Days to Ship as the difference between Ship Date and Order Date. If Days to Ship is 0, set Ship Mode to \"Same Day\"; if it is 7, set Ship Mode to \"Standard Class\"\n",
    "df['Days to Ship'] = (df['Ship Date'] - df['Order Date']).dt.days\n",
    "df.loc[df['Days to Ship'] == 0, 'Ship Mode'] = 'Same Day'\n",
    "df.loc[df['Days to Ship'] == 7, 'Ship Mode'] = 'Standard Class'\n",
    "print(\"Missing values in Ship Mode and Days to Ship have been imputed and updated accordingly.\")\n",
    "# Display the first few rows of the updated DataFrame\n",
    "print(df.head())    \n",
    "# Save the updated DataFrame to a new CSV file\n",
    "df.to_csv(\"../CaseStudy3/SuperStore_Updated_Dataset.csv\", index=False)\n",
    "print(\"Updated dataset saved to 'SuperStore_Updated_Dataset.csv'.\")\n",
    "\n",
    "#Impute missing values in the Quantity column using a method of your choice. Print the rationale for selecting the imputation method\n",
    "df['Quantity'] = df['Quantity'].fillna(df['Quantity'].median())\n",
    "print(\"Missing values in Quantity column have been imputed using the median value of the column, which is a robust method that minimizes the impact of outliers on the dataset.\")   \n",
    "# Display the first few rows of the DataFrame after imputation\n",
    "print(df.head())\n",
    "# Save the DataFrame with imputed Quantity to a new CSV file\n",
    "df.to_csv(\"../CaseStudy3/SuperStore_Quantity_Imputed_Dataset.csv\", index=False)\n",
    "print(\"Dataset with imputed Quantity saved to 'SuperStore_Quantity_Imputed_Dataset.csv'.\")  \n",
    "\n",
    "#Document the number of rows and distinct Order IDs affected by these operations\n",
    "affected_rows_quantity = df[df['Quantity'].isnull()].shape[0]\n",
    "affected_order_ids_quantity = df[df['Quantity'].isnull()]['Order ID'].nunique()\n",
    "print(f\"Number of affected rows in Quantity: {affected_rows_quantity}\") \n",
    "print(f\"Number of distinct affected Order IDs in Quantity: {affected_order_ids_quantity} \\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f27335",
   "metadata": {},
   "source": [
    "Data Masking and String Handling\n",
    "\n",
    "Drop the Customer Name column to protect Personal Identifiable Information (PII)\n",
    "Create a new column called Customer Name Masked, containing only the initials of the customer name\n",
    "\n",
    "Note: It's important to protect PII in datasets to maintain customer privacy and comply with data protection regulations. Masking or dropping sensitive data like customer names is a crucial step in this process\n",
    "Convert the Postal Code column from numeric to text format, ensuring all codes are 5 characters long. Add a leading '0' where necessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b02651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column called Customer Name Masked, containing only the initials of the customer name\n",
    "df['Customer Name Masked'] = df['Customer Name'].apply(lambda x: ''.join([name[0] for name in x.split()]))\n",
    "print(\"Customer Name Masked column has been created with initials of the customer names.\")\n",
    "\n",
    "# Drop the Customer Name column to protect Personal Identifiable Information (PII)\n",
    "df = df.drop(columns=['Customer Name'])\n",
    "print(\"Customer Name column has been dropped to protect Personal Identifiable Information (PII).\")\n",
    "\n",
    "# Convert the Postal Code column from numeric to text format, ensuring all codes are 5 characters long\n",
    "df['Postal Code'] = df['Postal Code'].astype(str).str.zfill(5)\n",
    "print(\"Postal Code column has been converted to text format with leading zeros where necessary.\")\n",
    "\n",
    "# Display the first few rows of the DataFrame after all modifications\n",
    "print(df.head())\n",
    "\n",
    "# Save the DataFrame with all modifications to a new CSV file\n",
    "df.to_csv(\"../CaseStudy3/SuperStore_Final_Dataset.csv\", index=False)\n",
    "print(\"Final dataset saved to 'SuperStore_Final_Dataset.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45033108",
   "metadata": {},
   "source": [
    "Data Type Conversion\n",
    "\n",
    "Convert the Quantity and Sales Price columns from strings to their appropriate numeric types (int and float, respectively)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cdb2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the Quantity and Sales Price columns from strings to their appropriate numeric types (int and float, respectively)\n",
    "df['Quantity'] = pd.to_numeric(df['Quantity'], errors='coerce')\n",
    "df['Sales Price'] = pd.to_numeric(df['Sales Price'], errors='coerce')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b8443f",
   "metadata": {},
   "source": [
    "Handling Inconsistent Categorical Data\n",
    "\n",
    "Clean the State column by replacing abbreviations with full state names (e.g., \"CA\" should be changed to \"California\"). You may need to research state abbreviations online to ensure all entries are corrected consistently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c168b56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the State column by replacing abbreviations with full state names (e.g., \"CA\" should be changed to \"California\"). You may need to research state abbreviations online to ensure all entries are corrected consistently\n",
    "state_abbreviations = {\n",
    "    'CA': 'California', \n",
    "    'NY': 'New York',\n",
    "    'TX': 'Texas',      \n",
    "    'FL': 'Florida',\n",
    "    'IL': 'Illinois',\n",
    "    'PA': 'Pennsylvania',\n",
    "    'OH': 'Ohio',       \n",
    "    'MI': 'Michigan',\n",
    "    'NJ': 'New Jersey',\n",
    "    'GA': 'Georgia',\n",
    "    'NC': 'North Carolina',\n",
    "    'VA': 'Virginia',\n",
    "    'WA': 'Washington',\n",
    "    'AZ': 'Arizona',\n",
    "    'MA': 'Massachusetts',  \n",
    "    'IN': 'Indiana',\n",
    "    'TN': 'Tennessee',\n",
    "    'MO': 'Missouri',\n",
    "    'MD': 'Maryland',\n",
    "    'WI': 'Wisconsin',\n",
    "    'CO': 'Colorado',\n",
    "    'MN': 'Minnesota',\n",
    "    'SC': 'South Carolina',\n",
    "    'AL': 'Alabama',\n",
    "    'LA': 'Louisiana',\n",
    "    'KY': 'Kentucky',\n",
    "    'OR': 'Oregon', \n",
    "    'OK': 'Oklahoma',\n",
    "    'CT': 'Connecticut',    \n",
    "    'IA': 'Iowa',\n",
    "    'KS': 'Kansas',\n",
    "    'AR': 'Arkansas',\n",
    "    'UT': 'Utah',\n",
    "    'NV': 'Nevada',\n",
    "    'MS': 'Mississippi',\n",
    "    'NM': 'New Mexico',\n",
    "    'WV': 'West Virginia',\n",
    "    'NE': 'Nebraska',\n",
    "    'ID': 'Idaho',\n",
    "    'HI': 'Hawaii',\n",
    "    'ME': 'Maine',\n",
    "    'NH': 'New Hampshire',\n",
    "    'RI': 'Rhode Island',\n",
    "    'VT': 'Vermont',\n",
    "    'DE': 'Delaware',\n",
    "    'SD': 'South Dakota',\n",
    "    'ND': 'North Dakota',\n",
    "    'WY': 'Wyoming',\n",
    "    'MT': 'Montana',\n",
    "    'AK': 'Alaska',\n",
    "    'DC': 'District of Columbia'\n",
    "}\n",
    "df['State'] = df['State'].replace(state_abbreviations)\n",
    "print(\"State abbreviations have been replaced with full state names.\")  \n",
    "# Display the first few rows of the DataFrame after state name correction\n",
    "print(df.head())\n",
    "# Save the DataFrame with corrected state names to a new CSV file\n",
    "df.to_csv(\"../CaseStudy3/SuperStore_State_Corrected_Dataset.csv\", index=False)\n",
    "print(\"Dataset with corrected state names saved to 'SuperStore_State_Corrected_Dataset.csv'.\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3b2bc",
   "metadata": {},
   "source": [
    "Feature Engineering\n",
    "\n",
    "Create new columns\n",
    "Original Price: The price before any discount is applied\n",
    "Total Sales: The total revenue is generated by multiplying the Sales Price by Quantity\n",
    "Total Profit: The total profit is earned by multiplying the Profit by Quantity\n",
    "Discount Price: The amount of discount applied, calculated based on the Original Price and Discount\n",
    "Total Discount: The total discount value for the quantity sold\n",
    "Create a new column, Shipping Urgency, based on Days to Ship\n",
    "If Days to Ship is 0, set to \"Immediate\"\n",
    "If Days to Ship is between 1 and 3, set to \"Urgent\"\n",
    "If Days to Ship is more than 3, set to \"Standard\"\n",
    "Create a column that calculates days since last order\n",
    "Create a new dataset that stores the total sales, quantity, and discount per customer, and then merge these back to the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e252961",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create new columns\n",
    "Original Price: The price before any discount is applied\n",
    "Total Sales: The total revenue is generated by multiplying the Sales Price by Quantity\n",
    "Total Profit: The total profit is earned by multiplying the Profit by Quantity\n",
    "Discount Price: The amount of discount applied, calculated based on the Original Price and Discount\n",
    "Total Discount: The total discount value for the quantity sold'''\n",
    "\n",
    "df['Original Price'] = df['Sales Price'] / (1 - df['Discount'])\n",
    "df['Total Sales'] = df['Sales Price'] * df['Quantity']\n",
    "df['Total Profit'] = df['Profit'] * df['Quantity']\n",
    "df['Discount Price'] = df['Original Price'] * df['Discount']    \n",
    "df['Total Discount'] = df['Discount Price'] * df['Quantity']\n",
    "# Display the first few rows of the DataFrame after adding new columns\n",
    "print(df.head())\n",
    "# Save the DataFrame with new columns to a new CSV file\n",
    "df.to_csv(\"../CaseStudy3/SuperStore_New_Columns_Dataset.csv\", index=False)\n",
    "print(\"Dataset with new columns saved to 'SuperStore_New_Columns_Dataset.csv'.\")\n",
    "\n",
    "\n",
    "\n",
    "'''Create a new column, Shipping Urgency, based on Days to Ship\n",
    "If Days to Ship is 0, set to \"Immediate\"\n",
    "If Days to Ship is between 1 and 3, set to \"Urgent\"\n",
    "If Days to Ship is more than 3, set to \"Standard\"'''\n",
    "df['Shipping Urgency'] = 'Standard'  # Default value\n",
    "df.loc[df['Days to Ship'] == 0, 'Shipping Urgency'] = 'Immediate'\n",
    "df.loc[(df['Days to Ship'] > 0) & (df['Days to Ship'] <= 3), 'Shipping Urgency'] = 'Urgent'\n",
    "# Display the first few rows of the DataFrame after adding Shipping Urgency\n",
    "print(df.head())\n",
    "# Save the DataFrame with Shipping Urgency to a new CSV file\n",
    "df.to_csv(\"../CaseStudy3/SuperStore_Shipping_Urgency_Dataset.csv\", index=False)\n",
    "print(\"Dataset with Shipping Urgency saved to 'SuperStore_Shipping_Urgency_Dataset.csv'.\")\n",
    "\n",
    "#Create a column that calculates days since last order\n",
    "df['Days Since Last Order'] = (pd.to_datetime('today') - df['Order Date']).dt.days\n",
    "# Display the first few rows of the DataFrame after adding Days Since Last Order\n",
    "print(df.head())\n",
    "# Save the DataFrame with Days Since Last Order to a new CSV file\n",
    "df.to_csv(\"../CaseStudy3/SuperStore_Days_Since_Last_Order_Dataset.csv\", index=False)\n",
    "print(\"Dataset with Days Since Last Order saved to 'SuperStore_Days_Since_Last_Order_Dataset.csv'.\")\n",
    "\n",
    "#Create a new dataset that stores the total sales, quantity, and discount per customer, and then merge these back to the original dataset\n",
    "customer_summary = df.groupby('Customer ID').agg({\n",
    "    'Total Sales': 'sum',\n",
    "    'Quantity': 'sum',\n",
    "    'Discount': 'mean'\n",
    "}).reset_index()    \n",
    "# Rename columns for clarity\n",
    "customer_summary.columns = ['Customer ID', 'Total Sales per Customer', 'Total Quantity per Customer', 'Average Discount per Customer']\n",
    "# Merge the customer summary back to the original DataFrame\n",
    "df = df.merge(customer_summary, on='Customer ID', how='left')\n",
    "# Display the first few rows of the DataFrame after merging customer summary\n",
    "print(df.head())\n",
    "# Save the DataFrame with customer summary to a new CSV file\n",
    "df.to_csv(\"../CaseStudy3/SuperStore_Customer_Summary_Dataset.csv\", index=False)\n",
    "print(\"Dataset with customer summary saved to 'SuperStore_Customer_Summary_Dataset.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4af175a",
   "metadata": {},
   "source": [
    "Outlier Detection and Handling\n",
    "\n",
    "Identify and handle outliers in the Sales Price column\n",
    "Create a function called remove_outliers that takes as an argument the dataframe and the column that needs to be searched for outliers. Using the 3 * IQR rule, the function should detect and remove the outliers to return the cleaned dataframe\n",
    "Now use the function to detect outliers and remove outliers from the Sales Price and Profit columns\n",
    "\n",
    "Why 3*IQR?\n",
    "The 3 IQR method is applied in situations where the dataset has a high variance, and the standard 1.5 IQR might flag too many points as outliers. This method ensures that only the most extreme values are removed, preserving the integrity of the dataset while still mitigating the influence of true outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05359fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify and handle outliers in the Sales Price column\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "# Calculate Z-scores for Sales Price\n",
    "df['Sales Price Z-Score'] = np.abs(stats.zscore(df['Sales Price'], nan_policy='omit'))\n",
    "# Define a threshold for outliers (e.g., Z-score > 3)   \n",
    "outlier_threshold = 3\n",
    "df['Is Outlier'] = df['Sales Price Z-Score'] > outlier_threshold\n",
    "# Display the first few rows of the DataFrame after identifying outliers\n",
    "print(df.head())\n",
    "# Save the DataFrame with outlier information to a new CSV file\n",
    "df.to_csv(\"../CaseStudy3/SuperStore_Outliers_Dataset.csv\", index=False)\n",
    "print(\"Dataset with outlier information saved to 'SuperStore_Outliers_Dataset.csv'.\")\n",
    "\n",
    "#Create a function called remove_outliers that takes as an argument the dataframe and the column that needs to be searched for outliers. Using the 3 * IQR rule, the function should detect and remove the outliers to return the cleaned dataframe\n",
    "def remove_outliers(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 3 * IQR\n",
    "    upper_bound = Q3 + 3 * IQR\n",
    "    cleaned_df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "    return cleaned_df\n",
    "df_cleaned = remove_outliers(df, 'Sales Price')\n",
    "# Display the first few rows of the cleaned DataFrame after removing outliers\n",
    "print(df_cleaned.head())\n",
    "# Save the cleaned DataFrame without outliers to a new CSV file\n",
    "df_cleaned.to_csv(\"../CaseStudy3/SuperStore_Cleaned_Outliers_Dataset.csv\", index=False)\n",
    "print(\"Cleaned dataset without outliers saved to 'SuperStore_Cleaned_Outliers_Dataset.csv'.\")\n",
    "\n",
    "\n",
    "'''Now use the function to detect outliers and remove outliers from the Sales Price and Profit columns\n",
    "\n",
    "Why 3*IQR?\n",
    "The 3 IQR method is applied in situations where the dataset has a high variance, and the standard 1.5 IQR might flag too many points as outliers. This method ensures that only the most extreme values are removed, preserving the integrity of the dataset while still mitigating the influence of true outliers\n",
    "'''\n",
    "\n",
    "df_cleaned_sales_price = remove_outliers(df, 'Sales Price')\n",
    "df_cleaned_profit = remove_outliers(df, 'Profit')\n",
    "# Display the first few rows of the cleaned DataFrame after removing outliers from Sales Price\n",
    "print(df_cleaned_sales_price.head())\n",
    "# Save the cleaned DataFrame without outliers from Sales Price to a new CSV file\n",
    "df_cleaned_sales_price.to_csv(\"../CaseStudy3/SuperStore_Cleaned_Sales_Price_Outliers_Dataset.csv\", index=False)\n",
    "print(\"Cleaned dataset without Sales Price outliers saved to 'SuperStore_Cleaned_Sales_Price_Outliers_Dataset.csv'.\")\n",
    "# Display the first few rows of the cleaned DataFrame after removing outliers from Profit\n",
    "print(df_cleaned_profit.head())\n",
    "# Save the cleaned DataFrame without outliers from Profit to a new CSV file\n",
    "df_cleaned_profit.to_csv(\"../CaseStudy3/SuperStore_Cleaned_Profit_Outliers_Dataset.csv\", index=False)\n",
    "print(\"Cleaned dataset without Profit outliers saved to 'SuperStore_Cleaned_Profit_Outliers_Dataset.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5273903d",
   "metadata": {},
   "source": [
    "Customer Segmentation and Analysis\n",
    "\n",
    "Calculate Customer Sales Quintile and Customer Profit Quintile based on total sales and total profit per Customer ID\n",
    "What is a Quintile? Quintiles are a statistical way of dividing data into five equal parts, each representing 20% of the data. For example, customers in the top quintile (Q5) represent the top 20% of sales or profit\n",
    "Create a cross-grid (cross-tabulation) based on these two quintiles to analyze the relationship between customer sales and profitability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79dc6801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate total sales and profit per customer\n",
    "customer_summary = df.groupby('Customer ID').agg({\n",
    "    'Total Sales': 'sum',\n",
    "    'Total Profit': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate quintiles on the aggregated data\n",
    "customer_summary['Total Sales Quintile'] = pd.qcut(customer_summary['Total Sales'], 5, labels=[1, 2, 3, 4, 5])\n",
    "customer_summary['Total Profit Quintile'] = pd.qcut(customer_summary['Total Profit'], 5, labels=[1, 2, 3, 4, 5])\n",
    "\n",
    "# Merge quintile info back to the main DataFrame\n",
    "df = df.merge(customer_summary[['Customer ID', 'Total Sales Quintile', 'Total Profit Quintile']], on='Customer ID', how='left')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())\n",
    "\n",
    "# Save the DataFrame with quintiles\n",
    "df.to_csv(\"../CaseStudy3/SuperStore_Quintiles_Dataset.csv\", index=False)\n",
    "print(\"Dataset with quintiles saved to 'SuperStore_Quintiles_Dataset.csv'.\")\n",
    "\n",
    "# Create cross-grid\n",
    "cross_grid = pd.crosstab(df['Total Sales Quintile'], df['Total Profit Quintile'], margins=True, margins_name='Total')\n",
    "print(\"Cross-grid of Customer Sales Quintile and Customer Profit Quintile:\")\n",
    "print(cross_grid)\n",
    "cross_grid.to_csv(\"../CaseStudy3/SuperStore_Cross_Grid_Quintiles.csv\")\n",
    "print(\"Cross-grid saved to 'SuperStore_Cross_Grid_Quintiles.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f7f275",
   "metadata": {},
   "source": [
    "Final Analysis and Dashboard Creation\n",
    "\n",
    "Sales and Profit Analysis\n",
    "\n",
    "Top 10 Most Profitable Products: Use a bar chart to display the products with the highest total profit\n",
    "Top 10 Most Loss-Making Products: Use a bar chart to display the products with the highest total losses (negative profit)\n",
    "Sales vs. Profit Correlation: Use a scatter plot to visualize the correlation between Total Sales and Total Profit. Add a regression line to show the trend\n",
    "Joint Distribution of Sales and Profit: Create a joint plot to analyze the relationship between Total Sales and Total Profit across different products\n",
    "Customer Segmentation and Analysis\n",
    "\n",
    "Customer Sales Quintile vs. Customer Profit Quintile: Create a heatmap or cross-tabulation to explore the relationship between customers' sales and profit quintiles. This will help identify which segments of customers are most valuable\n",
    "Understand how different product categories perform across customer segments: Create a pivot table to analyze the total Sales and total Profit by Category and Segment. Sort the pivot table to highlight the most profitable and least profitable combinations of Category and Segment\n",
    "Shipping and Delivery Analysis\n",
    "\n",
    "Distribution of Shipping Urgency: Visualize the distribution of orders by Shipping Urgency using a pie chart or bar chart\n",
    "Days to Ship vs. Profit: Use a violin plot to explore the distribution of Profit across different Days to Ship categories. This will help analyze whether faster shipping correlates with higher or lower profitability\n",
    "Shipping Mode and Profitability: Create a grouped bar chart to compare the profitability of different shipping modes (e.g., Standard Class, First Class)\n",
    "Using a pivot table, determine which shipping modes are most preferred across different regions and analyze the impact on total sales and profit. Create a pivot table that shows the count of Order IDs, total Sales, and total Profit for each Region and Ship Mode. Identify and print your insights\n",
    "Regional Sales and Profitability\n",
    "\n",
    "Sales and Profit by Region: Use a map or bar chart to visualize total sales and profit by region or state. This will highlight which regions are the most profitable\n",
    "\n",
    "State-wise Profitability: Create a pivot table to summarize the profitability of each state. Highlight the top and bottom states based on profitability\n",
    "\n",
    "Correlation between State and Profit: Use a correlation plot to identify any patterns or relationships between the states and the profitability of orders\n",
    "\n",
    "(Hint: Convert the categorical 'State' column into numerical values using label encoding using “from sklearn.preprocessing import LabelEncoder”)\n",
    "\n",
    "Discount and Pricing Analysis\n",
    "\n",
    "Impact of Discounts on Profitability: Use a scatter plot with a trend line to analyze how different levels of discount affect profitability\n",
    "Original Price vs. Discounted Price: Create a line plot to compare the original price and the discounted price across various product categories or sub-categories\n",
    "Temporal Analysis\n",
    "\n",
    "Sales and Profit Trends Over Time: Use a time series plot to analyze how sales and profit have trended over the years or months. This will help in identifying any seasonal patterns\n",
    "Order Frequency by Month: Use a bar chart or line plot to show the number of orders placed each month. Highlight any months with unusually high or low order frequencies\n",
    "Yearly Growth in Sales and Profit: Use a year-over-year growth chart to compare the sales and profit growth over different years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf828a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Top 10 Most Profitable Products\n",
    "top_10_profitable_products = df.groupby('Product Name')['Total Profit'].sum().nlargest(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_10_profitable_products.plot(kind='bar', color='skyblue')\n",
    "plt.title('Top 10 Most Profitable Products')\n",
    "plt.xlabel('Product Name')\n",
    "plt.ylabel('Total Profit')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top 10 Most Loss-Making Products\n",
    "top_10_loss_making_products = df.groupby('Product Name')['Total Profit'].sum().nsmallest(10)\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_10_loss_making_products.plot(kind='bar', color='salmon')\n",
    "plt.title('Top 10 Most Loss-Making Products')\n",
    "plt.xlabel('Product Name')\n",
    "plt.ylabel('Total Profit')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sales vs. Profit Correlation\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['Total Sales'], df['Total Profit'], alpha=0.5, color='purple')\n",
    "plt.title('Sales vs. Profit Correlation')\n",
    "plt.xlabel('Total Sales')\n",
    "plt.ylabel('Total Profit')\n",
    "sns.regplot(x='Total Sales', y='Total Profit', data=df, scatter=False, color='red')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Joint Distribution of Sales and Profit\n",
    "g = sns.jointplot(x='Total Sales', y='Total Profit', data=df, kind='scatter', color='green')\n",
    "g.fig.suptitle('Joint Distribution of Sales and Profit', y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Customer Sales Quintile vs. Customer Profit Quintile Heatmap\n",
    "cross_tab_quintiles = pd.crosstab(df['Total Sales Quintile'], df['Total Profit Quintile'], margins=True, margins_name='Total')\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cross_tab_quintiles, annot=True, fmt='d', cmap='YlGnBu', cbar_kws={'label': 'Count'})\n",
    "plt.title('Customer Sales Quintile vs. Customer Profit Quintile')\n",
    "plt.xlabel('Customer Profit Quintile')\n",
    "plt.ylabel('Customer Sales Quintile')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pivot Table: Total Sales and Profit by Category and Segment\n",
    "pivot_table = df.pivot_table(values=['Total Sales', 'Total Profit'], index='Category', columns='Segment', aggfunc='sum')\n",
    "pivot_table = pivot_table.sort_values(by=('Total Profit', 'Consumer'), ascending=False)\n",
    "print(\"Pivot Table of Total Sales and Total Profit by Category and Segment:\")\n",
    "print(pivot_table)\n",
    "pivot_table.to_csv(\"../CaseStudy3/SuperStore_Pivot_Table_Category_Segment.csv\")\n",
    "print(\"Pivot table saved to 'SuperStore_Pivot_Table_Category_Segment.csv'.\")\n",
    "\n",
    "# Distribution of Shipping Urgency\n",
    "shipping_urgency_counts = df['Shipping Urgency'].value_counts()\n",
    "plt.figure(figsize=(8, 6))\n",
    "shipping_urgency_counts.plot(kind='bar', color='lightcoral')\n",
    "plt.title('Distribution of Shipping Urgency')\n",
    "plt.xlabel('Shipping Urgency')\n",
    "plt.ylabel('Number of Orders')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Days to Ship vs. Profit (Violin Plot)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='Days to Ship', y='Total Profit', data=df, palette='muted')\n",
    "plt.title('Days to Ship vs. Profit Distribution')\n",
    "plt.xlabel('Days to Ship')\n",
    "plt.ylabel('Total Profit')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Shipping Mode and Profitability\n",
    "shipping_mode_profit = df.groupby('Ship Mode')['Total Profit'].sum().sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "shipping_mode_profit.plot(kind='bar', color='teal')\n",
    "plt.title('Shipping Mode and Profitability')\n",
    "plt.xlabel('Shipping Mode')\n",
    "plt.ylabel('Total Profit')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Pivot Table: Shipping Modes by Region\n",
    "pivot_table_shipping_region = df.pivot_table(\n",
    "    values=['Order ID', 'Total Sales', 'Total Profit'],\n",
    "    index='Region',\n",
    "    columns='Ship Mode',\n",
    "    aggfunc={'Order ID': 'count', 'Total Sales': 'sum', 'Total Profit': 'sum'}\n",
    ")\n",
    "print(\"Pivot Table of Order Count, Total Sales, and Total Profit by Region and Ship Mode:\")\n",
    "print(pivot_table_shipping_region)\n",
    "pivot_table_shipping_region.to_csv(\"../CaseStudy3/SuperStore_Pivot_Table_Shipping_Region.csv\")\n",
    "print(\"Pivot table saved to 'SuperStore_Pivot_Table_Shipping_Region.csv'.\")\n",
    "\n",
    "print(\"\\nInsights from the pivot table:\")\n",
    "pt_dict = pivot_table_shipping_region.to_dict()\n",
    "for region in pivot_table_shipping_region.index:\n",
    "    for ship_mode in pivot_table_shipping_region.columns.get_level_values(0).unique():\n",
    "        order_count = pt_dict.get((ship_mode, 'Order ID'), {}).get(region, 0)\n",
    "        total_sales = pt_dict.get((ship_mode, 'Total Sales'), {}).get(region, 0)\n",
    "        total_profit = pt_dict.get((ship_mode, 'Total Profit'), {}).get(region, 0)\n",
    "        print(f\"In {region}, the {ship_mode} shipping mode has {order_count} orders with total sales of ${total_sales:.2f} and total profit of ${total_profit:.2f}.\")\n",
    "\n",
    "affected_rows_shipping = df[df['Ship Mode'].isnull()].shape[0]\n",
    "affected_order_ids_shipping = df[df['Ship Mode'].isnull()]['Order ID'].nunique()\n",
    "print(f\"Number of affected rows in Ship Mode: {affected_rows_shipping}\")\n",
    "print(f\"Number of distinct affected Order IDs in Ship Mode: {affected_order_ids_shipping} \\n\")\n",
    "\n",
    "# State-wise Profitability\n",
    "state_profitability = df.groupby('State')['Total Profit'].sum().sort_values(ascending=False)\n",
    "plt.figure(figsize=(12, 6))\n",
    "state_profitability.plot(kind='bar', color='lightgreen')\n",
    "plt.title('State-wise Profitability')\n",
    "plt.xlabel('State')\n",
    "plt.ylabel('Total Profit')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation between State and Profit (Label Encoding)\n",
    "df['State_encoded'] = LabelEncoder().fit_transform(df['State'])\n",
    "correlation_matrix = df[['State_encoded', 'Total Profit']].corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation between State and Profit')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Impact of Discounts on Profitability\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Discount', y='Total Profit', data=df, alpha=0.5, color='orange')\n",
    "sns.regplot(x='Discount', y='Total Profit', data=df, scatter=False, color='red')\n",
    "plt.title('Impact of Discounts on Profitability')\n",
    "plt.xlabel('Discount')\n",
    "plt.ylabel('Total Profit')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Original Price vs. Discounted Price by Category\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='Original Price', y='Sales Price', data=df, hue='Category', palette='Set1')\n",
    "plt.title('Original Price vs. Discounted Price by Category')\n",
    "plt.xlabel('Original Price')\n",
    "plt.ylabel('Sales Price')\n",
    "plt.legend(title='Category', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Sales and Profit Trends Over Time\n",
    "df['Order Date'] = pd.to_datetime(df['Order Date'], errors='coerce')\n",
    "monthly_trends = df.groupby(df['Order Date'].dt.to_period('M')).agg({'Total Sales': 'sum', 'Total Profit': 'sum'}).reset_index()\n",
    "monthly_trends['Order Date'] = monthly_trends['Order Date'].dt.to_timestamp()\n",
    "plt.figure(figsize=(14, 7))\n",
    "plt.plot(monthly_trends['Order Date'], monthly_trends['Total Sales'], label='Total Sales', color='blue')\n",
    "plt.plot(monthly_trends['Order Date'], monthly_trends['Total Profit'], label='Total Profit', color='green')\n",
    "plt.title('Sales and Profit Trends Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Amount')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Order Frequency by Month\n",
    "monthly_order_frequency = df['Order Date'].dt.to_period('M').value_counts().sort_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "monthly_order_frequency.plot(kind='bar', color='purple')\n",
    "plt.title('Order Frequency by Month')\n",
    "plt.xlabel('Month')\n",
    "plt.ylabel('Number of Orders')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Yearly Growth in Sales and Profit\n",
    "yearly_growth = df.groupby(df['Order Date'].dt.year).agg({'Total Sales': 'sum', 'Total Profit': 'sum'}).reset_index()\n",
    "yearly_growth.rename(columns={'Order Date': 'Year'}, inplace=True)\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(yearly_growth['Year'], yearly_growth['Total Sales'], label='Total Sales', marker='o', color='blue')\n",
    "plt.plot(yearly_growth['Year'], yearly_growth['Total Profit'], label='Total Profit', marker='o', color='green')\n",
    "plt.title('Yearly Growth in Sales and Profit')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Amount')\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
